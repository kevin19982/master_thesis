---
title: "master_thesis_data_preparation"
output: html_notebook
---

```{r set up}
# clean up environment
rm(list = ls())


# load packages
library(readxl)
library(dplyr)
library(usefun)
library(tidyr)
library(ggplot2)
library(plyr)


# load in data
data <- read_excel("data/Data_CarbonNeutral_compact.xlsx") 





```


Now it is time to filter the data for variables that are relevant for the
analysis, as there are a lot of variables that describe technical things
regarding the questionnaire, which do not provide information for the question
at hand.
```{r filter for relevant variables}
# create a list of variables that are relevant for the analysis
colnames(data)
var_list <- c("lfdn", "purchase_CHO", "purchase_PA", "purchase_GB",
                 "GB_S1", "GB_S2", "GB_S3", "GB_S4", "GB_S5", "GB_S6", "GB_S7", 
                 "GB_S8", "GB_S9", "PA_S1", "PA_S2", "PA_S3", "PA_S4", "PA_S5", 
                 "PA_S6", "PA_S7", "PA_S8", "PA_S9", "CHO_S1", "CHO_S2", 
                 "CHO_S3", "CHO_S4", "CHO_S5", "CHO_S6", "CHO_S7", "CHO_S8", 
                 "CHO_S9", "rank_GB", "rank_CHO", "rank_CHEESE", "rank_WHEAT",
                 "rank_TOM", "age", "gender", "country", "education", 
                 "occupation", "av_income", "environment", "Country_Name",
                 "Generation", "final_comment", "browser", "datetime",
                 "date_of_last_access", "date_of_first_mail", "Date_Start",
                 "Date_End", "Time_Start", "Time_End", "Completion_Time")

data_clean <- data[,var_list]

#print("")
print(paste0("all variables: ",dim(data)[2], ", variables for the analysis: ", 
             dim(data_clean)[2]))

```


```{r recode na}
# replace non-answer with NA's
data_clean[data_clean == -66 | data_clean == -77 | data_clean == -99] <- NA

```




```{r select observations}
# select observations that worked on the questionnaire for a reasonable time
#data_clean$Completion_Time


# function to calculate completion times, technically they are in the given
# data set, but don't account for transitions between dates
time_passed  <- function(dataset){
  # initiate column for completion time in seconds
  dataset$Completion_Time_sec = rep(NA, dim(dataset)[1])
  
  # calculate difference between Time_End and Time_Start in seconds, add seconds
  # for each day passed between start and finish time
  for (i in 1:dim(dataset)[1]){
    dataset$Completion_Time_sec = (dataset$Time_End - dataset$Time_Start) +
      (dataset$Date_End - dataset$Date_Start)
  }
  
  # make sure the column is numeric
  dataset$Completion_Time_sec <- as.numeric(dataset$Completion_Time_sec)
  
  return(dataset)
}


# apply function
data_clean <- time_passed(data_clean)


# check results
data_clean$Completion_Time_sec[1:10]
# table(data_clean$Completion_Time_sec)



# pre-select observations based on completion time
# long completion times will not be excluded, as participants might have been
# distracted but faithfully finished the questionnaire later
# very short completion times will be excluded, as participants might have not
# payed attention
# the approach might be greedy, as data-hungry methods incentivize keeping as
# many observations as possible, the author accounted for this by deep 
# philosophical contemplation of one's motives and biases... and a coin flip

# check dimension before filtering
print(paste0("number of observations before filtering: ", dim(data_clean)[1]))

# filter data set
data_clean <- data_clean[data_clean$Completion_Time_sec >= 120,]
# minimum time: 120 seconds, which seems reasonable when accounting for
# different questionnaire constellations, reading speeds, and decision making

# check dimensions after filtering
print(paste0("number of observations after filtering: ", dim(data_clean)[1]))

```



```{r worldmap}
# show map of where the answers are from (Costa Rica should be as visible as
# possible, since there were relatively a lot of observations from there, but
# it's a small land)
unique(data_clean$country)
unique(data_clean$Country_Name)


# get dataframe with countries and number of participants
participants_origin <- count(data_clean, "Country_Name")
participants_origin

# adjust country names to the ones ggplot uses
participants_origin$Country_Name[participants_origin$Country_Name == "United Kingdom"] <- "UK"
participants_origin$Country_Name[participants_origin$Country_Name == "United States"] <- "USA"


# filter world data
world_data <- map_data("world") %>% filter(region != "Antarctica") %>% fortify
# no one from Antarctica participated... I'm shocked as well

# print map
ggplot() +
  geom_map(data = world_data, map = world_data,
           aes(long, lat, map_id = region), fill = "white", colour = "black",
           size = 0.5) +
  geom_map(data = participants_origin, map = world_data,
           aes(fill = freq, map_id = Country_Name), colour = "black") +
  scale_fill_continuous(low = "cornflowerblue", high = "midnightblue") +
  xlab("longitudinal") + ylab("lateral") +
  theme_bw()

```



```{r worldmap zoom in}
# check country names
#sort(unique(world_data$region))

# countries for European map (not only European countries)
europe_country_list = c("Albania, Andorra", "Austria", "Belarus", "Belgium", 
                        "Bosnia and Herzegovina", "Bulgaria", "Croatia", 
                        "Czech Republic", "Denmark", "Estonia", "Finland", 
                        "France", "Germany", "Greece", "Hungary", "Iceland",
                        "Italy", "Latvia", "Liechtenstein", "Lithuania", 
                        "Luxembourg", "Malta", "Moldova", "Monaco", 
                        "Montenegro", "Netherlands", "North Macedonia", 
                        "Norway", "Poland", "Portugal", "Romania", "Russia", 
                        "San Marino", "Serbia", "Slovakia", "Slovenia", "Spain", 
                        "Sweden", "Switzerland", "Ukraine", "UK", 
                        "Turkey", "Cyprus", "Syria", "Lebanon", "Israel", 
                        "Jordan", "Morocco", "Tunesia", "Lybia", "Algeria", 
                        "Egypt", "Georgia", "Azerbaijan", "Iraq")
europe <- world_data[world_data$region == europe_country_list,]


# print map
map_europe <- ggplot() +
  geom_map(data = europe, map = world_data,
           aes(long, lat, map_id = region), fill = "white", colour = "black",
           size = 0.5) + 
  geom_map(data = participants_origin, map = world_data, 
           aes(fill = freq, map_id = Country_Name), colour = "black") +
  scale_fill_continuous(low = "cornflowerblue", high = "midnightblue") +
  xlab("longitudinal") + ylab("lateral") +
  theme_bw() + xlim(-10,40) + ylim(35,70)
print(map_europe)


# countries for north- and middle-american map (not only north-and middle-
# american countries)
america_nm_country_list <- c("Canada", "Greendland", "Mexico", "USA",
                             "Antigua", "Bahamas", "Barbados", "Cuba", 
                             "Dominica", "The Dominican Republic", "Grenada", 
                             "Haiti", "Saint Kitts", "Saint Lucia", 
                             "Saint Vincent", "Trinidad",
                             "Costa Rica", "El Salvador", "Guatemala", 
                             "Honduras", "Nicaragua", "Panama",
                             "Colombia", "Venezuela", "Guyana", "Suriname")
america_nm <- world_data[world_data$region == america_nm_country_list,]
# it is also doable with only zooming in and out, but that would require more
# trial and error with the limits

# print map
map_america_nm <- ggplot() +
  geom_map(data = america_nm, map = world_data,
           aes(long, lat, map_id = region), fill = "white", colour = "black",
           size = 0.5) + 
  geom_map(data = participants_origin, map = world_data, 
           aes(fill = freq, map_id = Country_Name), colour = "black") +
  scale_fill_continuous(low = "cornflowerblue", high = "midnightblue") +
  xlab("longitudinal") + ylab("lateral") +
  theme_bw() + xlim(-175,-25) + ylim(10, 80)
print(map_america_nm)


# countries for south-american map (not only south-american countries)
america_s_country_list <- c("Argentina", "Bolivia", "Brazil", "Chile", 
                            "Colombia", "Eucador", "Guyana", "Paraguay", "Peru", 
                            "Suriname", "Uruguay", "Venezuela",
                            "Mexico", "Costa Rica", "El Salvador", "Guatemala", 
                            "Honduras", "Nicaragua", "Panama")
america_s <- world_data[world_data$region == america_s_country_list,]

# print map
map_america_s <- ggplot() +
  geom_map(data = america_s, map = world_data,
           aes(long, lat, map_id = region), fill = "white", colour = "black",
           size = 0.5) + 
  geom_map(data = participants_origin, map = world_data, 
           aes(fill = freq, map_id = Country_Name), colour = "black") +
  scale_fill_continuous(low = "cornflowerblue", high = "midnightblue") +
  xlab("longitudinal") + ylab("lateral") +
  theme_bw() + xlim(-105, -20) + ylim(-55,15)
print(map_america_s)

```






```{r create objects}
# 3 different data sets for chocolate, pasta and ground beef
# chocolate
data_choc <- subset(data_clean, purchase_CHO == 1)
# pasta
data_pasta <- subset(data_clean, purchase_PA == 1)
# ground beef
data_grouf <- subset(data_clean, purchase_GB == 1)

print(paste0("number of observations chocolate: ", dim(data_choc)[1]))
print(paste0("number of observations pasta: ", dim(data_pasta)[1]))
print(paste0("number of observations ground beef: ", dim(data_grouf)[1]))

#print("")
print(paste0("share of participants not buying chocolate: ", 
             round((dim(data)[1] - dim(data_choc)[1]) / dim(data)[1], 4) 
             * 100, "%"))
print(paste0("share of participants not buying pasta: ", 
             round((dim(data)[1] - dim(data_pasta)[1]) / dim(data)[1], 4)
             * 100, "%"))
print(paste0("share of participants not buying ground beef: ", 
             round((dim(data)[1] - dim(data_grouf)[1]) / dim(data)[1], 4)
             * 100, "%"))



```
There can be models run separately for each of the datasets regarding the
product choices as only those that bought a certain product were given question
about said product. Pasta keeps the most observations as only about 15% of 
participants do not buy pasta, hence, it might be the best dataset for 
data hungry algorithms like neural nets.




```{r filter variables for each dataset}
# choice-variables do not need to be kept for other products... or perhaps
# they do have an influence, but in this case, "-77" needs to be recoded
data_choc <- data_choc %>% select(!contains("PA_") & !contains("GB_"))
print(paste0("number of variables chocolate: ", dim(data_choc)[2]))
# expect 35 variables

data_pasta <- data_pasta %>% select(!contains("CHO_") & !contains("GB_"))
print(paste0("number of variables pasta: ", dim(data_pasta)[2]))

data_grouf <- data_grouf %>% select(!contains("PA_") & !contains("CHO_"))
print(paste0("number of variables ground beef: ", dim(data_grouf)[2]))

```





```{r check na-frequency - chocolate}
# function to get NA's of a dataframe for each variable
na_perc <- function(dataset){
  na_s <- data.frame(colnames(dataset))
  na_s["NA_freq"] <- rep(NA, dim(dataset)[2])
  for (i in 1:dim(dataset)[2]){
    na_s[i, 2] <- round(sum(is.na(dataset[,i])) / dim(dataset)[1], 4) * 100 
  }
  return(na_s)
}


# print results
# the table displays the relative frequency of NA's for each variable
# na-frequencies chocolate dataset
na_perc(data_choc)

```

```{r check na-frequency - pasta}
# na-frequencies pasta dataset
na_perc(data_pasta)

```



```{r check na-frequency - ground beef}
# na-frequencies ground beef dataset
na_perc(data_grouf)

```



```{r table choice-variables - chocolate}
# examine choice-variables
#table(data_choc$CHO_S1, useNA = "always")

# function to table all variables of a dataset
table_values <- function(dataset){
  for (i in 1:dim(dataset)[2]){
    #print(colnames(dataset[,i]))
    print(table(dataset[,i], useNA = "always"))
    print_empty_line()
  }
}


# check choice-variables
# choice-distribution chocolate 
table_values(data_choc %>% select(matches("_S[[:digit:]]+")))


```



```{r table choice-variables - pasta}
# choice-distribution pasta
table_values(data_pasta %>% select(matches("_S[[:digit:]]+")))




```


```{r tables choice-variables - ground beef}
# choice-distribution ground beef
table_values(data_grouf %>% select(matches("_S[[:digit:]]+")))



```


```{r create variable indicating environmenttal decisions}
# recode choice-variables so for every variable the second option is the one
# with the carbon neutral label
recode_vars <- function(dataset_choice, variables){
  dataset_choice <- dataset_choice %>%
    mutate_at(variables, funs(recode(., '1' = 2, '2' = 1, '3' = 3, 
                                     .default = NaN)))
  return(dataset_choice)
}



# check distribution of one variable for comparison
print("CHO_S3 before recoding:")
print(table(data_choc$CHO_S3))
print("")



# recode dataset for chocolate
data_choc <- recode_vars(data_choc, c("CHO_S3", "CHO_S5", "CHO_S7", "CHO_S8", 
                                      "CHO_S9"))

# recode dataset for pasta
data_pasta <- recode_vars(data_pasta, c("PA_S3", "PA_S5", "PA_S7", "PA_S8",
                                        "PA_S9"))

# recode dataset for groundbeef
data_grouf <- recode_vars(data_grouf, c("GB_S3", "GB_S5", "GB_S7", "GB_S8",
                                        "GB_S9"))



# check distribution for one variable after recoding
print("CHO_S3 after recoding:")
print(table(data_choc$CHO_S3))




# build variable indicating environmental consciousness
build_env_cons_var <- function(dataset, product){
  # create column for result
  dataset["environmental_consciousness_indicator"] <- rep(NA, dim(dataset)[1])
  # subset data for relevant variables
  dataset_choice <- dataset %>% select(matches(paste0(product, 
                                                      "_S[[:digit:]]+")))
  
  # subset dataset for categories of choices
  dataset_choice_same_price <- dataset_choice %>% select(matches(paste0(product, 
                                                      "_S[1, 6, 9]")))
  dataset_choice_cn_more_expensive <- dataset_choice %>% 
    select(matches(paste0(product, "_S[2, 7]")))
  
  dataset_choice_cn_more_expensive_extreme <- dataset_choice %>% 
    select(matches(paste0(product, "_S[3]")))
  
  dataset_choice_cn_less_expensive <- dataset_choice %>% 
    select(matches(paste0(product, "_S[4, 8]")))
  
  dataset_choice_cn_less_expensive_extreme <- dataset_choice %>% 
    select(matches(paste0(product, "_S[5]")))
  
  
  # check
  print("data subsets:")
  print(product)
  print(colnames(dataset_choice_same_price))
  print(colnames(dataset_choice_cn_more_expensive))
  print(colnames(dataset_choice_cn_more_expensive_extreme))
  print(colnames(dataset_choice_cn_less_expensive))
  print(colnames(dataset_choice_cn_less_expensive_extreme))
  print("")
  
  
  

  # recode variables to match the score the particular answer gives
  
  print(paste0("same price before ", product, ":"))
  print(lapply(dataset_choice_same_price, table))
  
  
  cols_same_price <- colnames(dataset_choice_same_price)
  dataset_choice_same_price <- dataset_choice_same_price %>%
    mutate_at(cols_same_price, funs(recode(., '1' = -1, '2' = 1, '3' = 0, 
                                           .default = NaN)))
  
  print(paste0("same price after ", product, ":"))
  print(lapply(dataset_choice_same_price, table))
  
  
  
  print(paste0("cn more expensive before ", product, ":"))
  print(lapply(dataset_choice_cn_more_expensive, table))
  
  
  
  cols_cn_more_expensive <- colnames(dataset_choice_cn_more_expensive)
  dataset_choice_cn_more_expensive <- dataset_choice_cn_more_expensive %>%
    mutate_at(cols_cn_more_expensive, funs(recode(., '1' = 0, '2' = 2, 
                                                  '3' = 0, .default = NaN)))
  
  
  
  print(paste0("cn more expensive after ", product, ":"))
  print(lapply(dataset_choice_cn_more_expensive, table))
  
  
  print(paste0("cn more expensive extreme before ", product, ":"))
  print(lapply(dataset_choice_cn_more_expensive_extreme, table))
  
  
  
  cols_cn_more_expensive_extreme <- 
    colnames(dataset_choice_cn_more_expensive_extreme)
  dataset_choice_cn_more_expensive_extreme <- 
    dataset_choice_cn_more_expensive_extreme %>% 
    mutate_at(cols_cn_more_expensive_extreme, funs(recode(., '1' = 0, '2' = 3, 
                                                          '3' = 0, 
                                                          .default = NaN))) 
  
    
  print(paste0("cn more expensive extreme after ", product, ":"))
  print(lapply(dataset_choice_cn_more_expensive_extreme, table))
  
  
  print(paste0("cn less expensive before ", product, ":"))
  print(lapply(dataset_choice_cn_less_expensive, table))
  
  
  
  cols_cn_less_expensive <- colnames(dataset_choice_cn_less_expensive)
  dataset_choice_cn_less_expensive <- dataset_choice_cn_less_expensive %>%
    mutate_at(cols_cn_less_expensive, funs(recode(., '1' = -2, '2' = 0, 
                                                  '3' = 0, .default = NaN)))
  
      
  print(paste0("cn less expensive after ", product, ":"))
  print(lapply(dataset_choice_cn_less_expensive, table))
  
  
  print(paste0("cn less expensive extreme before ", product, ":"))
  print(lapply(dataset_choice_cn_less_expensive_extreme, table))
  
  
  cols_cn_less_expensive_extreme <- 
    colnames(dataset_choice_cn_less_expensive_extreme)
  dataset_choice_cn_less_expensive_extreme <- 
    dataset_choice_cn_less_expensive_extreme %>%
    mutate_at(cols_cn_less_expensive_extreme, funs(recode(., '1' = -3, '2' = 0,
                                                   '3' = 0, .default = NaN)))
  
  
  print(paste0("cn less expensive extreme after ", product, ":"))
  print(lapply(dataset_choice_cn_less_expensive_extreme, table))
  print("")
  print("=====================================================================")
  
  # calculate indicator
  dataset["environmental_consciousness_indicator"] = 
    rowSums(dataset_choice_same_price, na.rm = TRUE) + 
    rowSums(dataset_choice_cn_more_expensive, na.rm = TRUE) +
    rowSums(dataset_choice_cn_more_expensive_extreme, na.rm = TRUE) +
    rowSums(dataset_choice_cn_less_expensive, na.rm = TRUE) + 
    rowSums(dataset_choice_cn_less_expensive_extreme, na.rm = TRUE)
  
  return(dataset)
}

# build indicator variable for chocolate dataset
data_choc <- build_env_cons_var(data_choc, "CHO")
data_pasta <- build_env_cons_var(data_pasta, "PA")
data_grouf <- build_env_cons_var(data_grouf, "GB")



# check functions again to make sure they make sense and work as intended
# and check whether the data encoding is correct




```






```{r visualize distributions}
# histograms of environmental consciousness indicator by dataset
# chocolate
hist(data_choc$environmental_consciousness_indicator, col = "cornflowerblue",
     main = "Environmental consciousness indicator distribution chocolate",
     xlim = c(-10, 10), xlab = "environmental consciousness indicator",
     ylab = "absolute frequency")

# pasta
hist(data_pasta$environmental_consciousness_indicator, col = "cornflowerblue",
     main = "Environmental consciousness indicator distribution pasta",
     xlim = c(-10, 10), xlab = "environmental consciousness indicator",
     ylab = "absolute frequency")

# groundbeef
hist(data_grouf$environmental_consciousness_indicator, col = "cornflowerblue",
     main = "Environmental consciousness indicator distribution ground beef",
     xlim = c(-10, 10), xlab = "environmental consciousness indicator",
     ylab = "absolute frequency")



# all conversions examined, given the system at hand, the distributions should
# be sound



```


```{r factor analysis}
# check whether similar choices were made

# answers already recoded to have the same direction (recode_vars-function)

# does factor analysis make sense for the type of data?

library(psych)
library(GPArotation)
library(nFactors)



# get an estimate of number of factors
evs_choc <- eigen(cor(na.omit(data_choc %>% 
                                select(matches("_S[[:digit:]]+")))))
ap_choc <- parallel(subject=nrow(data_choc %>% 
                                   select(matches("_S[[:digit:]]+"))), 
                    var=ncol(data_choc %>% select(matches("_S[[:digit:]]+"))), 
                    rep=100, cent=0.5)
nsr_choc <- nScree(x=evs_choc$values, apparallel=ap_choc$eigen$qevpea)
print(plotnScree(nsr_choc))

# 2-3 factors
choc_fa <- factanal(data_choc %>% select(matches("_S[[:digit:]]+")), 
                    factors = 3)
print(choc_fa)
print(choc_fa$uniquenesses)


# using another method
library(psych)

print("")
print("=========================================")
choc_fa_2 <- fa(data_choc %>% select(matches("_S[[:digit:]]+")), nfactors = 3,
                scores = "regression", rotate = "oblimin")
print(choc_fa_2$loadings)
print("")
print("loadings:")
print(choc_fa_2$uniquenesses)


# visualize first two factors
plot(choc_fa_2$loadings[,1], choc_fa_2$loadings[,2], 
     xlab= "1", ylab = "2", xlim = c(-1,1), ylim = c(-1,1), main = "chocolate")
text(choc_fa_2$loadings[,1] - 0.00, choc_fa_2$loadings[,2] + 0.20,
     colnames(data_choc %>% select(matches("_S[[:digit:]]+"))), 
     col = "cornflowerblue")
abline(h = 0, v = 0)



# similar answers for questions with same price; as well as questions for CN 
# more expensive; as well as question where CN less expensive





# pasta
# get an estimate of number of factors
evs_pasta <- eigen(cor(na.omit(data_pasta %>% 
                                 select(matches("_S[[:digit:]]+")))))
ap_pasta <- parallel(subject=nrow(data_pasta %>% 
                                    select(matches("_S[[:digit:]]+"))),
                     var=ncol(data_pasta %>% select(matches("_S[[:digit:]]+"))),
                     rep=100,cent=0.5)
nsr_pasta <- nScree(x=evs_pasta$values, apparallel=ap_pasta$eigen$qevpea)
print("")
print("======================================================================")
print("pasta:")
print(plotnScree(nsr_pasta))
pasta_fa_2 <- fa(data_pasta %>% select(matches("_S[[:digit:]]+")), nfactors = 2,
                scores = "regression", rotate = "oblimin")
print(pasta_fa_2$loadings)
print("")
print("loadings:")
print(pasta_fa_2$uniquenesses)


# visualize first two factors
plot(pasta_fa_2$loadings[,1], pasta_fa_2$loadings[,2], 
     xlab= "1", ylab = "2", xlim = c(-1,1), ylim = c(-1,1), main = "pasta")
text(pasta_fa_2$loadings[,1] - 0.00, pasta_fa_2$loadings[,2] + 0.20,
     colnames(data_pasta %>% select(matches("_S[[:digit:]]+"))), 
     col = "cornflowerblue")
abline(h = 0, v = 0)





# ground beef
evs_grouf <- eigen(cor(na.omit(data_grouf %>% 
                                 select(matches("_S[[:digit:]]+")))))
ap_grouf <- parallel(subject=nrow(data_grouf %>% 
                                    select(matches("_S[[:digit:]]+"))),
                     var=ncol(data_grouf %>% select(matches("_S[[:digit:]]+"))),
                     rep=100,cent=0.5)
nsr_grouf <- nScree(x=evs_grouf$values, apparallel=ap_grouf$eigen$qevpea)
print("")
print("======================================================================")
print("ground beef:")
print(plotnScree(nsr_grouf))
grouf_fa_2 <- fa(data_grouf %>% select(matches("_S[[:digit:]]+")), nfactors = 2,
                scores = "regression", rotate = "oblimin")
print(grouf_fa_2$loadings)
print("")
print("loadings:")
print(grouf_fa_2$uniquenesses)




# visualize first two factors
plot(grouf_fa_2$loadings[,1], grouf_fa_2$loadings[,2], 
     xlab= "1", ylab = "2", xlim = c(-1,1), ylim = c(-1,1), 
     main = "ground beef")
text(grouf_fa_2$loadings[,1] - 0.00, grouf_fa_2$loadings[,2] + 0.20,
     colnames(data_grouf %>% select(matches("_S[[:digit:]]+"))), 
     col = "cornflowerblue")
abline(h = 0, v = 0)







```



```{r exploratory factor analysis}
# function to conduct an exploratory factor analysis
get_factor_analysis_exploratory <- function(dataset, rotations){
  
  # subset data
  data_temp <- dataset %>% select(matches("_S[[:digit:]]+"))
  # choose number of factors
  evs_temp <- eigen(cor(na.omit(data_temp )))
  ap_temp <- parallel(subject = nrow(data_temp), var = ncol(data_temp),
                      rep = 100, cent = 0.5)
  nsr_temp <- nScree(x = evs_temp$values, apparallel = ap_temp$eigen$qevpea)
  
  # print the scree plot
  print(plotnScree(nsr_temp))
  
  
  # choose number of factors
  n_fact <- nsr_temp$Components$noc

  print(paste0("number of factors: ", n_fact))
  
  
  # factor analysis
  for (rotation in rotations){
    fa_temp <- fa(data_temp, nfactors = n_fact, scores = "regression",
                  rotate = rotation)
    
    # print results
    print(paste0(rotation, ":"))
    print("laodings:")
    print(fa_temp$loadings)
    print("uniquenesses:")
    print(fa_temp$uniquenesses)
    
    # visualize first two factors
    if (n_fact > 1){
      plot(fa_temp$loadings[,1], fa_temp$loadings[,2],
           xlab = "1", ylab = "2", xlim = c(-1, 1), ylim = c(-1, 1),
           main = paste0("first two factors ", rotation))
      text(fa_temp$loadings[,1] - 0.00, fa_temp$loadings[,2] + 0.20,
           colnames(data_temp), col = "cornflowerblue")
      abline(h = 0, v = 0)
    }
  }
}

# specify rotations
fa_rotations = list("none", "varimax", "promax", "oblimin")


# chocolate
get_factor_analysis_exploratory(data_choc, fa_rotations)


# pasta
get_factor_analysis_exploratory(data_pasta, fa_rotations)


# ground beef
get_factor_analysis_exploratory(data_grouf, fa_rotations)

```






```{r regression}
# regression analysis






```




```{r random forest}

# also add a directed acylyic graph at some point... would be a lovely detail




```






```{r learners}





```






```{r neural net}






```






